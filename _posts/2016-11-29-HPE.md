---
layout: post
title: "Using Power BI Embedded to help Hewlett Packard Enterprise visualize counterfeit information"
author: "Anders Gill, Julien Stroheker, Beat Schwegler"
author-link: "#"
#author-image: "../images/hpe/andersgill.jpg"
date: 2016-11-29
categories: [Power BI Embedded]
color: "blue"
#image: "{{ site.baseurl }}/images/imagename.png" #should be ~350px tall
excerpt: During a hackfest with Microsoft, Hewlett Packard Enterprise implemented a Power BI Embedded report for its cloud-based anti-counterfeit service to better visualize where and how many times a wide range of products has been authenticated.
language: English
verticals: [Risk Management, Analytics]
---

![Logo of Hewlett Packard Enterprise]({{ site.baseurl }}/images/hpe/hpelogo.png)

Hewlett Packard Enterprise (HPE) fights against fraudulent goods; their cloud-based product-authentication service helps brand owners fight counterfeiting and the diversion of products to unintended markets. HPE added Microsoft Power BI Embedded to their solution to gather intelligence and to give a better overview of the counterfeiting situation.

This report describes the technology choices made and the learnings from the hackfest.

### Key technologies used ###

To achieve the goals, HPE and Microsoft used the following technologies:

- Power BI Embedded for data visualization in web application
  - Power BI Desktop for creating the initial report
- Microsoft Azure
  - Azure SQL Database for storing data
  - Power BI Embedded resource to create the initial Power BI Workspace Collection
  - Web Apps feature of Azure for hosting the proof of concept (PoC)
- [PowerShell](https://github.com/PowerShell/PowerShell), the [Power BI command-line interface](https://github.com/Microsoft/PowerBI-Cli) (Power BI CLI), and the [Azure cross-platform command-line interface](https://github.com/Azure/azure-xplat-cli) (Azure CLI) to manage and automate the deployment

### Hackfest members ###

The following people participated in the hackfest:

- Lisa Finn – Data Scientist, HPE
- Beat Schwegler ([@cloudbeatsch](https://twitter.com/cloudbeatsch)) – Principal Software Development Engineer, Microsoft
- Shweta Gupta ([shweta-gupta-6a29574](https://www.linkedin.com/in/shweta-gupta-6a29574/) on LinkedIn) – Senior Software Development Engineer, Microsoft
- Julien Stroheker ([@ju_stroh](https://twitter.com/ju_stroh)) – Technical Evangelist, Microsoft
- Anders Gill ([@therealshadyman](https://twitter.com/therealshadyman)) – Technical Evangelist, Microsoft

## Customer profile ##

[Hewlett Packard Enterprise](https://www.hpe.com/us/en/home.html) has been in the innovation business for more than 75 years; it split from Hewlett-Packard (now HP Inc.) in 2015. Headquartered in Palo Alto, California, HPE focuses on enterprises, primarily within technology services and products, consulting, and financial services.

## Problem statement ##

One major challenge for HPE is the growth of counterfeit products and the diversion of these products to unintended markets. Customers think that they are buying official HPE products when, in reality, they are buying counterfeit products. The effect on HPE is strongly negative: They lose sales and reputation when the quality of the counterfeited products doesn't meet the requirements of HPE.

The HPE answer to this challenge is the Global Product Authentication Service (GPAS), a cloud-based anti-counterfeit service that was first fielded in Africa to protect the antimalarial drug of a non-governmental organization (NGO). In May 2012, the Printing and Imaging division of HP used this tool to protect its toner and ink cartridge products by using a proprietary algorithm to create high numbers of unique, randomized codes that are printed on the packaging. This allows consumers to authenticate products by scanning the printed QR code with a smartphone or texting an SMS version of the same code to the GPAS service. The response back is simply "authentic", "not authentic", "subject to a recall", or any other messages the brand owner wants to communicate to the end customer (consumer).

The brand owners are then able to track the product authentications with minimal latency on the cloud-based GPAS dashboard based on the IP addresses. This makes it much easier for the brand owner to recall faulty products precisely, rather than broadly.

HPE started this journey with the goal of using Power BI Embedded to deliver the visualizations for the GPAS Dashboard. They engaged with Microsoft to deliver a working *minimum viable product* (MVP).

The challenges in the project were as follows:

- Finding a secure, scalable way to deliver and store data
- Automating the provisioning of the Power BI Workspace Collection in Azure
- Data cleansing and preprocessing

### Preparing for the hackfest ###

A few weeks before the hackfest, HPE prepared a custom data-frame extraction from the production database so that we would have data ready to work with. A day before the hackfest, HPE sent a brief so that the Microsoft team was on the same page regarding features for the GPAS reporting solution.

Because the Microsoft team had little information about the back end of the existing solution, the first couple of hours of the hackfest went into mapping the architecture in the solution and ensuring that the scope of the project was achievable within a two-day hackfest.

## Solution ##

The project had two main objectives:

- Automate the process of provisioning Power BI Workspaces in Azure by implementing DevOps practices
- Embed a Power BI report into a web application with the data provided by the demo data frame

Because the project was fairly ambitious based on the time allocated, we decided to split the project into smaller projects:

- Clean the data from the source and prepare it for the appropriate data model
- Create a web application that could embed the finished report
- Create the script needed for automation of all the resources on Azure: Power BI Collection, Workspace, and Web Apps

When all of the smaller projects were completed, we could merge it together to create one solution as a PoC.

### Power BI Embedded automation ###

When you want to begin using Power BI Embedded on Azure, there are a few steps to take by using either the [Azure portal](https://portal.azure.com/) or the Power BI CLI to set up an environment, such as the following:

- Create a Power BI Collection on Azure (portal)
- Create a Workspace in this Collection (CLI)
- Import a .PBIX file (CLI)

Then, if you want to be able to visualize your report embedded in a custom web application, you must insert the correct reference (a GUID) in your Power BI Collection, Workspace, and Report.

All those steps could be tedious, especially if you do it repeatedly for testing. During development, design, or testing of your reports, you don't want to waste time to do all those steps manually through the portal. You want to quickly deploy and see your change in your environment.

In the spirit to implement DevOps practices around automation with Azure and the Power BI services, during this hackfest we worked with HPE on a PowerShell script ([julienstroheker/PBIEmbDeploy](https://github.com/julienstroheker/PBIEmbDeploy) on GitHub) to empower them to

- Install automatically all prerequisites on a local environment to work with Azure and Power BI by installing the necessary CLI
- Be able to easily redeploy or update their environments with no differences between them, including the Power BI resources but also the web application code if necessary
- Be able to run this script on Unix or Windows
- Apply the new parameters generated by Power BI, such as the workspace ID or report ID, to the custom web app by using the application parameters settings in Azure
- Be able to modify this script by releasing it as open source on GitHub

At a high level, this is the common architecture that should generally be used in a Power BI Embedded workflow:

![Architecture for Power BI Embedded workflow]({{ site.baseurl }}/images/hpe/PBIAutoFlow.png)

Thanks to the PowerShell script, people from HPE can now focus on development and not waste time on deployment and updating. For example, each time someone wants to deploy and see the result of a report in a real environment, he or she can run the following script:

```bash
/deploy.ps1 -ResourceGroupName "myResourceGroupOnAzure" -Location "West US" -PrefixName "Contoso123" -PrefixNameEnv "Demo456"
```

In the future, we can think about more automation and DevOps practices by adding a tool like Visual Studio Team Services and do the following:

- **Continuous integration** Trigger this code every time someone pushes a new version of the .pbix file
- **Release management** Manage multiple environments, such as integration, staging, and production
- **Automated tests** Using Selenium, check whether we have the correct visuals and data by automating user interaction
  
### Data cleansing ###

The data frame that HPE extracted from the production database was already located in Azure SQL Database. After connecting to it with SQL Server Management Studio (SSMS), we could properly understand the columns and rows of numbers in the DB.

Based on this information, we knew that some columns needed to be properly cast into the appropriate data types. After thinking about our data-scrub strategy, we cast some of the columns from string to int/numeric to be able to count on the data. We scrubbed numerics, removed any columns that weren't relevant for the reporting, and ended with removing some empty rows to increase the overall quality of the data.

Because the data frame was just a test, it was more important to educate ourselves on why and what to do to increase the overall quality of the data so that HPE can effectively evaluate whether this is something that they need to conduct in production.

#### Missing dimension ####

Because the number of observations (rows) in the production dataset is a couple of million (and still increasing), showing all observations when the report loads would be inefficient. Therefore, we implemented a "top-level filter" to drill down and show only subsets of the data based on the selection.

There are many ways of filtering the data, but because HPE wants to see data based on time, a timeline visual would be the most beneficial in this case. A timeline visual is something that requires a column with the DateTime datatype. The test dataset did indeed have this, but not as a DateTime datetype. So we went back into the database and cast the numeric time series into a DateTime datatype. This change allowed us to populate the timeline.

When working with big amounts of data, there are some best practices to follow. The data table that contains the observations from HPE (also called measures) is referred to as the "fact table" because it contains business facts. These columns can act as a foreign key that refers to the candidate key (primary key) in another table, which consists only of unique values related to the foreign keys. This second table is usually referred to as the dimension table, based on the descriptive attributes (or fields).

In our case, there was no dimension table available for the time series in the database. If we were to implement the current solution without creating a dimension table, the time-series visual would have been populated incorrectly. So we decided to create this dimension based on a simple M-query in Power BI after we had connected and imported the data from the database in Power BI Desktop.

```csharp
//Create Date Dimension
(StartDate as date, EndDate as date)=>
let
    //Capture the date range from the parameters
    StartDate = #date(Date.Year(StartDate), Date.Month(StartDate), 
    Date.Day(StartDate)),
    EndDate = #date(Date.Year(EndDate), Date.Month(EndDate), 
    Date.Day(EndDate)),
//Get the number of dates that will be required for the table
    GetDateCount = Duration.Days(EndDate - StartDate),
//Take the count of dates and turn it into a list of dates
    GetDateList = List.Dates(StartDate, GetDateCount, 
    #duration(1,0,0,0)),
//Convert the list into a table
    DateListToTable = Table.FromList(GetDateList, 
    Splitter.SplitByNothing(), {"Date"}, null, ExtraValues.Error),
//Create various date attributes from the date column
    //Add Year Column
    YearNumber = Table.AddColumn(DateListToTable, "Year", 
    each Date.Year([Date])),
//Add Quarter Column
    QuarterNumber = Table.AddColumn(YearNumber , "Quarter", 
    each "Q" & Number.ToText(Date.QuarterOfYear([Date]))),
//Add Week Number Column
    WeekNumber= Table.AddColumn(QuarterNumber , "Week Number", 
    each Date.WeekOfYear([Date])),
//Add Month Number Column
    MonthNumber = Table.AddColumn(WeekNumber, "Month Number", 
    each Date.Month([Date])),
//Add Month Name Column
    MonthName = Table.AddColumn(MonthNumber , "Month", 
    each Date.ToText([Date],"MMMM")),
//Add Day of Week Column
    DayOfWeek = Table.AddColumn(MonthName , "Day of Week", 
    each Date.ToText([Date],"dddd"))
in
    DayOfWeek 
```

#### Data model ####

Following is a screenshot of the data model. Hackfest_Authentication_Log_Sample2 is the fact table, and Dates2 effectively works as the dimension table with only unique dates based on the M-query. Table1 (dimension) also consists of unique values based on validity and a corresponding picture based on the status of the authentication observation. 

![Data model]({{ site.baseurl }}/images/hpe/datamodel.png)

### Web application ###

The custom Power BI Embedded web application that implements the report for HPE is available from GitHub: [cloudbeatsch/PowerBI-Embedded-Site](https://github.com/cloudbeatsch/PowerBI-Embedded-Site).

You can use this app to implement your own report. The only thing you need to provide to get it to work is to define the following AppSettings (either in web config or in WebApp):

```
powerbi:AccessKey
powerbi:WorkspaceCollection
powerbi:WorkspaceId
```

*The web application result*

![Report in the web application]({{ site.baseurl }}/images/hpe/webapp.png)

### Data visualization ###

Following is an image of the PoC report built atop various custom visualizations from the Power BI [Visuals Gallery](https://app.powerbi.com/visuals). Here you can see that we are using the data for 2013 Q3 to output the details about all authentications within that period and visualize them on the ArcGIS Map visual.

*The report in Power BI Desktop*

![Report in Power BI Desktop]({{ site.baseurl }}/images/hpe/report.png)

To provide the conditioning for the images based on the state of the authentication in the URL column, we created a simple DAX function:

```csharp
Validity = IF(NOT(ISBLANK(Hackfest_Authentication_Log_Sample2[Label_Id]))&&(Hackfest_Authentication_Log_Sample2[Label_Id] <> "0")&&(Hackfest_Authentication_Log_Sample2[Authentication_Result] = "TRUE"),"Valid code",IF(NOT(ISBLANK(Hackfest_Authentication_Log_Sample2[Label_Id]))&&(Hackfest_Authentication_Log_Sample2[Label_Id] <> "0")&&(Hackfest_Authentication_Log_Sample2[Authentication_Result] = "FALSE"), "valid failure code", "invalid"))
```

The pseudocode for the DAX is as follows:

```csharp
IF [Label Id] IS NOT NULL AND [Label Id] IS NOT THE NUMBER 0 AND [Authentication Result] EQUALS TRUE THEN OUTPUT 'Valid Code' ELSE IF [Label Id] IS NOT NULL AND [Label Id] IS NOT THE NUMBER 0 AND [Authentication Result] EQUALS FALSE THEN OUTPUT 'Valid Failure Code' ELSE OUTPUT 'Invalid Code'
```

## Conclusion ##

Implementing a solution similar to the PoC for HPE can improve operational efficiencies. The solution allows HPE to move the focus from building custom visualizations and charts with popular libraries such as D3 or HighCharts, and to avoid potential hazards that might follow, such as the costs of development, testing, debugging, and maintenance during their lifetimes. This life cycle might end up costing much more than implementing a solution like we did for HPE, which still gives room for flexibility in case they want to create custom visualizations later.

### Lessons learned ###

- DevOps practices can be employed with Power BI Embedded. Doing so can empower data scientists to focus only on the data instead of resolving the IT problems of deployment and tests. For example, we can easily imagine a company manually deploying the same dashboard multiple times a day (which is highly inefficient).
- Drawing sketches on paper can help you visualize how you want the end result to appear, even though the paper sketch is low fidelity.
- It is important to properly clean the data and have a proper data-scrubbing strategy to increase the quality of the overall data before using it for reporting purposes and visualization.
- Leveraging the right tools to solve the given problem is key to ending up with the result you aim for. 
- Having a proper plan before you actually execute is key to removing bottlenecks that might arise. This also includes planning which people in the team should work on each project and how the projects should be consolidated later.   

### Going forward ###

HPE is strongly considering to implement this solution in their production site to leverage the opportunities that Power BI enables. We experienced working closely with Power BI and a full-fledged data pipeline. One vision that HPE has is to create visually stunning 3D maps where they can plot the data on custom maps that suit their needs.

## Resources ##

- [HPE Global Product Authentication Service](http://www8.hp.com/us/en/software-solutions/product-authentication-anti-counterfeit-services/)
- [Power BI Embedded sample dashboard](https://microsoft.github.io/PowerBI-JavaScript/demo/filters.html)
- [Power BI Ideas board](https://ideas.powerbi.com/forums/265200-power-bi-ideas)
- PowerShell automation script: [julienstroheker/PBIEmbDeploy](https://github.com/julienstroheker/PBIEmbDeploy)
- A Power BI Embedded sample that demonstrates how to integrate a Power BI report into your own web app: [Azure-Samples/power-bi-embedded-integrate-report-into-web-app](https://github.com/Azure-Samples/power-bi-embedded-integrate-report-into-web-app)
- [Power BI CLI](https://github.com/Microsoft/PowerBI-Cli)
- [Azure CLI](https://github.com/Azure/azure-xplat-cli)
- [PowerShell](https://github.com/PowerShell/PowerShell)
